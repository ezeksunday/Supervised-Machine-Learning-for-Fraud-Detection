# -*- coding: utf-8 -*-
"""fraud_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vteaOUKtziIGWMOEmlPVxIP7ErbEsbaH

# Required Libraries
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler,MinMaxScaler,LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, classification_report
import warnings
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential,Model
from tensorflow.keras.layers import Dense,Flatten,Dropout,SimpleRNN,LSTM,Input,Reshape
from sklearn.feature_selection import SelectKBest,f_classif
from imblearn.over_sampling import SMOTE
from sklearn.tree import DecisionTreeClassifier
warnings.filterwarnings("ignore")

"""# Data Importation and Exploration"""

df=pd.read_csv('naija_fraud.csv')

df.head()

df.shape

df.info()

df=df.drop(['AcountNumber','CVV','CardInformation'],axis=1)

df=df.dropna()

df.info()

df.Outcome.value_counts()

plt.figure(figsize=(18,9))
df.Outcome.value_counts().plot.bar(color='black')

df.head()

df= pd.get_dummies(df, columns=['Gender','Marital Status','Cards','CardColour','CardType','TransactionType','Domain'], drop_first=True)

df

df.info()

"""# Data Preprocessing"""

plt.figure(figsize=(15,7))
sns.countplot('Outcome',data=df)

"""# Data Splitting"""

x = df.drop(['Outcome'],axis=1)
y = df.Outcome

"""# Feature Selection"""

select=SelectKBest(score_func=f_classif,k=5)
z=select.fit_transform(x,y)
flt=select.get_support()
s=x.columns
print(s[flt])

x=df[['CreditLimit', 'Amount', 'AverageIncomeExpendicture', 'NewBalance',
       'TransactionType_Debit']]

"""# Data Balancing"""

smote=SMOTE()
x_resampled,y_resampled=smote.fit_resample(x,y)
scaled = StandardScaler()
x_resampled = scaled.fit_transform(x_resampled)
u,v=np.unique(y_resampled,return_counts=True)
for x in range(len(u)):
  print(u[x],'=',v[x])

"""# Training and Testing splits"""

x_train_resampled,x_test,y_train_resampled,y_test=train_test_split(x_resampled,y_resampled,test_size=.3,random_state=42)

"""# Model Development and Evaluations

## FFNN MODEL
"""

model=Sequential()
model.add(Dense(124,activation='relu',input_shape=(5,)))
model.add(Dense(64,activation='relu'))
model.add(Flatten())
model.add(Dense(64,activation='relu'))
model.add(Flatten())
model.add(Dense(64,activation='relu'))
model.add(Dense(32,activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(1, activation='sigmoid'))
model.summary()
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
his=model.fit(x_train_resampled,y_train_resampled,epochs=100,batch_size=64,validation_data=(x_test, y_test))
y_pred=model.predict(x_test)
y_pred=(y_pred>=0.5).astype(int)

plt.plot(his.history['loss'])
plt.plot(his.history['val_loss'])
plt.legend(['actual','predicted'])
plt.title('Model loss')
plt.ylabel('loss')
plt.show()

plt.plot(his.history['accuracy'])
plt.plot(his.history['val_accuracy'])
plt.legend(['actual','predicted'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.show()

print(accuracy_score(y_pred, y_test))
print(f1_score(y_pred, y_test))
# print(classification_report(y_pred, y_test))
plt.figure(figsize=(10,6))
fx=sns.heatmap(confusion_matrix(y_test,y_pred),annot=True,fmt='.0f')
fx.set_xlabel('PREDICTED VALUES')
fx.set_ylabel('ACTUAL VALUES')
fx.xaxis.set_ticklabels(['Fraudulent', 'Non-Fraudulent'])
fx.yaxis.set_ticklabels(['Fraudulent', 'Non-Fraudulent'])
plt.show()

x_train_resampled=x_train_resampled.reshape(x_train_resampled.shape[0],x_train_resampled.shape[1],1)
x_test=x_test.reshape(x_test.shape[0],x_test.shape[1],1)

x_test.shape

"""## LSTM MODEL"""

lstm_model=Sequential()
lstm_model.add(LSTM(128,input_shape=(x_train_resampled.shape[1],x_train_resampled.shape[2])))
lstm_model.add(Dense(64,activation='relu'))
lstm_model.add(Flatten())
lstm_model.add(Dense(64,activation='relu'))
lstm_model.add(Flatten())
lstm_model.add(Dense(64,activation='relu'))
lstm_model.add(Dense(32,activation='relu'))
lstm_model.add(Dropout(0.2))
lstm_model.add(Dense(1, activation='sigmoid'))
lstm_model.summary()
lstm_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
his=lstm_model.fit(x_train_resampled,y_train_resampled,epochs=100,batch_size=64,validation_data=(x_test, y_test))
y_pred=lstm_model.predict(x_test)
y_pred=(y_pred>=0.5).astype(int)

plt.plot(his.history['loss'])
plt.plot(his.history['val_loss'])
plt.legend(['actual','predicted'])
plt.title('Model loss')
plt.ylabel('loss')
plt.show()

plt.plot(his.history['accuracy'])
plt.plot(his.history['val_accuracy'])
plt.legend(['actual','predicted'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.show()

print(accuracy_score(y_pred, y_test))
print(f1_score(y_pred, y_test))
# print(classification_report(y_pred, y_test))
plt.figure(figsize=(10,6))
fx=sns.heatmap(confusion_matrix(y_test,y_pred),annot=True,fmt='.0f')
fx.set_xlabel('PREDICTED VALUES')
fx.set_ylabel('ACTUAL VALUES')
fx.xaxis.set_ticklabels(['Fraudulent', 'Non-Fraudulent'])
fx.yaxis.set_ticklabels(['Fraudulent', 'Non-Fraudulent'])
plt.show()

